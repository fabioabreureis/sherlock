#!/bin/bash

readonly BENCHMARK_PROGRAM=${1}
readonly JOB_TYPE=${2}
readonly RUN_NAME=${3}

# workload related
readonly WORKLOAD_RUNTIME=600
readonly THREADS=10
readonly CLIENTS=10
readonly DB_POD_PREFIX=postgresql
readonly DB_TYPE=pgsql
readonly OUTPUT_INTERVAL=10
readonly DB_USERNAME=redhat
readonly DB_PASSWORD=redhat
readonly DB_NAME=redhat
readonly SYSBENCH_NUMBER_OF_TABLES=400
#readonly SYSBENCH_ROWS_IN_TABLE=10000
readonly SYSBENCH_ROWS_IN_TABLE=1000000
readonly SYSBENCH_NUMBER_OF_INSERTS=1
readonly SYSBENCH_NUMBER_OF_UPDATES=1
readonly SYSBENCH_NUMBER_OF_NON_INDEX_UPDATES=1
readonly SYSBENCH_READ_ONLY=off
readonly SYSBENCH_WRITE_ONLY=off
readonly PGBENCH_WORKLOAD_TYPE=70r30w # simple, 90r10w, 70r30w, or readonly
readonly PGBENCH_RUN_TYPE=time
readonly PGBENCH_RUN_TYPE_VAR=${WORKLOAD_RUNTIME}
readonly PGBENCH_VACUUM=yes
readonly PGBENCH_QUIET=no
readonly PGBENCH_SCALE=7000
readonly PGBENCH_READ_ONLY=no

# cluster related
readonly KUBE_CMD=oc
readonly NUMBER_OF_WORKERS=2
readonly DB_PER_WORKER=9
readonly OCS_DEVICES="nvme1n1 nvme2n1 nvme3n1 nvme4n1 nvme6n1 nvme7n1 nvme8n1 nvme9n1"
readonly OCS_NETWORK_INTERFACES="enp216s0f0"
readonly STATS_INTERVAL=10
readonly STATS_COUNT=$(((WORKLOAD_RUNTIME+10)/STATS_INTERVAL))
readonly WORKERS_LIST_FILE=~/workers
readonly OCS_LIST_FILE=~/ocs_nodes
readonly PROJECT_NAME=postgresql
readonly STATS=true


declare -A jobs_pods
if [[ "${WORKERS_LIST_FILE}" == "${OCS_LIST_FILE}" ]]; then
  mapfile -t worker_node_array < ${WORKERS_LIST_FILE}
else
  mapfile -t worker_node_array < ${WORKERS_LIST_FILE}
  mapfile -t ocs_node_array < ${OCS_LIST_FILE}
fi

# TODO: find a sane way to find out what rbd device is being used for the postgresql pod.
# this is just a hardcoded  workaround that need to be updated whenever creating new databases
declare -A worker_psql_rbd
worker_psql_rbd[worker0.ocp4.example.com]="rbd0 rbd1 rbd2 rbd3 rbd4 rbd5 rbd6 rbd7 rbd8"
worker_psql_rbd[worker1.ocp4.example.com]="rbd0 rbd1 rbd2 rbd3 rbd4 rbd5 rbd6 rbd7 rbd8"
#worker_psql_rbd[worker0.ocp4.example.com]="rbd0 rbd1 rbd2 rbd3 rbd4 rbd5 rbd6 rbd7 rbd8 rbd9 rbd10 rbd11 rbd12 rbd13 rbd14 rbd15 rbd16 rbd17 rbd18 rbd19"
#worker_psql_rbd[worker1.ocp4.example.com]="rbd0 rbd1 rbd2 rbd3 rbd4 rbd5 rbd6 rbd7 rbd8 rbd9 rbd10 rbd11 rbd12 rbd13 rbd14 rbd15 rbd16 rbd17 rbd18 rbd19"


if [[ $# -eq 0 || $# -ne 3 ]]; then
 echo "Usage : $0 <sysbench or pgbench> <job type: prepare, run or cleanup(sysbench only)> <directory to keep run logs>"
 exit 1
fi

function run_db_workload()
{
  local node_name=$(oc get node ${node_name} -o jsonpath='{.metadata.labels.kubernetes\.io/hostname}')
  local db_ip="${1}"
  local node_type="${2}"
  if [[ "${BENCHMARK_PROGRAM}" == "sysbench" ]]; then
    run_command="./run_sysbench ${JOB_TYPE} ${WORKLOAD_RUNTIME} ${THREADS} ${SYSBENCH_READ_ONLY} ${SYSBENCH_WRITE_ONLY} ${db_ip} ${OUTPUT_INTERVAL} ${SYSBENCH_ROWS_IN_TABLE} ${SYSBENCH_NUMBER_OF_TABLES} ${DB_TYPE} ${DB_USERNAME} ${DB_PASSWORD} ${DB_NAME} ${SYSBENCH_NUMBER_OF_INSERTS} ${SYSBENCH_NUMBER_OF_UPDATES} ${SYSBENCH_NUMBER_OF_NON_INDEX_UPDATES}"
  else
    if [[ "${JOB_TYPE}" == "prepare" ]]; then
      run_command="./run_pgbench ${JOB_TYPE} ${db_ip} ${CLIENTS} ${THREADS} ${PGBENCH_RUN_TYPE} ${PGBENCH_RUN_TYPE_VAR} ${PGBENCH_VACUUM} ${PGBENCH_QUIET} ${PGBENCH_SCALE} ${DB_USERNAME} ${DB_PASSWORD} ${DB_NAME} ${OUTPUT_INTERVAL} ${PGBENCH_READ_ONLY}"
    else
      run_command="./run_pgbench ${PGBENCH_WORKLOAD_TYPE} ${db_ip} ${CLIENTS} ${THREADS} ${PGBENCH_RUN_TYPE} ${PGBENCH_RUN_TYPE_VAR} ${PGBENCH_VACUUM} ${PGBENCH_QUIET} ${PGBENCH_SCALE} ${DB_USERNAME} ${DB_PASSWORD} ${DB_NAME} ${OUTPUT_INTERVAL} ${PGBENCH_READ_ONLY}"
    fi
  fi
  echo "run_command=${run_command}"
  job_name=$(cat <<EOF | ${KUBE_CMD} create -f -
apiVersion: batch/v1
kind: Job
metadata:
  generateName: ${BENCHMARK_PROGRAM}-${JOB_TYPE}-${deployment_name}-${RUN_NAME}-
spec:
  template:
    spec:
      restartPolicy: OnFailure
      nodeSelector:
        kubernetes.io/hostname: ${node_name}
      containers:
        - image: quay.io/sagyvolkov/benchmark-container:0.87
          name: ${BENCHMARK_PROGRAM}-${JOB_TYPE}
          resources:
            limits:
              memory: 2Gi
          command:
                - "bash"
                - "-c"
                - >
                  ${run_command}
EOF
)
job_name=$(echo ${job_name} | awk '{print $1}')
workload_pod_name=$(${KUBE_CMD} describe ${job_name} | grep "Created pod:" | awk -F "Created pod: " '{print $2}')
}

function run_stats()
{
  local node_name=$(oc get node ${node_name} -o jsonpath='{.metadata.labels.kubernetes\.io/hostname}')
  local node_type="${1}"
  job_name=$(cat <<EOF | ${KUBE_CMD} create -f -
apiVersion: batch/v1
kind: Job
metadata:
  generateName: stats-${RUN_NAME}-${node_type}-${node_name}-
spec:
  template:
    spec:
      hostNetwork: true
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/hostname: ${node_name}
      containers:
        - image: quay.io/sagyvolkov/stats-container:0.94
          name: stats-${RUN_NAME}
          resources:
            limits:
              memory: 64Mi
              cpu: 0.1
          command:
              - "bash"
              - "-c"
              - echo "${OCS_NETWORK_INTERFACES}" > /tmp/ocs_network_interfaces; echo "${OCS_DEVICES}" > /tmp/ocs_devices; ./run_all ${STATS_INTERVAL} ${STATS_COUNT} ${node_type}
EOF
)
job_name=$(echo ${job_name} | awk '{print $1}')
stats_pod_name=$(${KUBE_CMD} describe ${job_name} | grep "Created pod:" | awk -F "Created pod: " '{print $2}')
}

function cordon_control()
{
  local cordon_command=${1}
  local cordon_node=${2}
  if [[ "${cordon_command}" == "cordon" ]]; then
    [[ "${KUBE_CMD}" == "oc" ]] && ${KUBE_CMD} adm cordon ${cordon_node} || ${KUBE_CMD} cordon ${cordon_node}
  else
    [[ "${KUBE_CMD}" == "oc" ]] && ${KUBE_CMD} adm uncordon ${cordon_node} || ${KUBE_CMD} uncordon ${cordon_node}
  fi
}

function stats_collect()
{
  local node_type=${1}
  shift 1
  local local_array=("$@")
  for node_name in ${local_array[@]}
  do
    echo "Starting to collect stats on ${node_type} node ${node_name}..."
    echo run_stats "${node_type}" "${local_array[@]}"
    run_stats "${node_type}" "${local_array[@]}"
    jobs_pods[${job_name}]=${stats_pod_name}
  done
}

# TODO: move calculate_rbd_utilization into the stats container
function calculate_rbd_utilization()
{
  local file_name=${1}
  local rbd_device="${2}"
  for device_name in ${rbd_device}
  do
      let r_s=0
      let w_s=0
      let util=0
      let number_of_inputs=0
      while read x
      do
        r_s=$(echo "$r_s + $(echo ${x}|awk -F, '{print $1}')"|bc)
        w_s=$(echo "$w_s + $(echo ${x}|awk -F, '{print $2}')"|bc)
        util=$(echo "$util + $(echo ${x}|awk -F, '{print $3}')"|bc)
        let number_of_inputs++
      done < <(grep ${device_name} ${file_name} | awk '{print $2","$8","$23}')
      avg_r_s=$(echo "scale=2;$r_s/$number_of_inputs"|bc)
      avg_w_s=$(echo "scale=2;$w_s/$number_of_inputs"|bc)
      avg_util=$(echo "scale=2;$util/$number_of_inputs"|bc)
      echo "NODE: ${node_ip}, AVERAGE RBD device ${device_name} read/s: ${avg_r_s}, write/s: ${avg_w_s}, utilization%: ${avg_util}" >> ${file_name}
  done
}

if [[ "${JOB_TYPE}" == "run" && "${STATS}" == "true" ]]; then
  stats_collect worker ${worker_node_array[@]}
  [[ ${#ocs_node_array[@]} -ne 0 ]] && stats_collect ocs ${ocs_node_array[@]}
fi

#if [[ "${JOB_TYPE}" == "init" ]]; then
#  for node in ${worker_node_array[@]}
#  do
#   echo cordon_control cordon ${node}
#  done
#fi


for ((j=1; j<=${DB_PER_WORKER}*${NUMBER_OF_WORKERS}; j++))
do
  node_number=$((j%NUMBER_OF_WORKERS))
  #cordon_control uncordon ${worker_node_array[${node_number}]}
  node_name=${worker_node_array[${node_number}]}
  deployment_name=${DB_POD_PREFIX}-${j}
  if [[ "${DB_TYPE}" == "pgsql" ]]; then
    database_ip=$(${KUBE_CMD} get svc -n ${PROJECT_NAME} ${deployment_name} -o jsonpath='{.spec.clusterIP}')
  else
    database_ip=$(${KUBE_CMD} get pod -l app=${deployment_name} -o jsonpath='{.items[].status.podIP}')
  fi
  echo "Starting ${BENCHMARK_PROGRAM} job for ${JOB_TYPE} in deployment ${deployment_name} with database ip ${database_ip} ..."
  #sleep 5
  run_db_workload ${database_ip}
  jobs_pods[${job_name}]=${workload_pod_name}
  echo "$(date) - job ${job_name} is using ${BENCHMARK_PROGRAM} pod ${workload_pod_name}"
  #cordon_control cordon ${worker_node_array[${node_number}]}
done

#echo "uncordoning all nodes..."
#for node in ${worker_node_array[@]}
#do
# echo "node=${node}"
# cordon_control uncordon ${node}
#done

if [[ "${JOB_TYPE}" == "run" ]];then
  if [[ ! -d "${RUN_NAME}" ]]; then
    mkdir -p "$RUN_NAME"
  fi
  job_list=""
  for i in "${!jobs_pods[@]}"
  do
    job_list="${job_list} ${i}"
  done
  echo "job_list=${job_list}"
  echo "Waiting for jobs to complete ..."
  ${KUBE_CMD} wait --for=condition=complete ${job_list} --timeout=4000s
  echo "$(date) - Getting logs from all jobs ..."
  for i in "${!jobs_pods[@]}"
  do
    ${KUBE_CMD} logs ${jobs_pods[$i]} > ${RUN_NAME}/${jobs_pods[$i]}.log
  done
  for node_ip in ${!worker_psql_rbd[@]}
  do
    file_name=$(ls ${RUN_NAME}/stats-${RUN_NAME}-worker-${node_ip}*.log)
    rbd_device="${worker_psql_rbd[${node_ip}]}"
    calculate_rbd_utilization ${file_name} "${rbd_device}"
  done
fi
${KUBE_CMD} get jobs
